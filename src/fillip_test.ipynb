{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9834, 400) (9834,)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../data/dataset.npz',)\n",
    "X, y = dataset['X'], dataset['y']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axis = plt.subplots(17, 5, figsize=(12, 48))\n",
    "\n",
    "# for label in range(17):\n",
    "#     for i in range(5):\n",
    "#         axis[label, i].imshow(X[y == label][i+5].reshape(20, 20), vmin=0, vmax=255, cmap='gray')\n",
    "#         axis[label, i].set_xticks([])\n",
    "#         axis[label, i].set_yticks([])\n",
    "#         axis[label, i].set_title(f'Label {label}')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 627\n",
      "Label 1: 230\n",
      "Label 2: 525\n",
      "Label 3: 950\n",
      "Label 4: 500\n",
      "Label 5: 695\n",
      "Label 6: 912\n",
      "Label 7: 605\n",
      "Label 8: 427\n",
      "Label 9: 205\n",
      "Label 10: 825\n",
      "Label 11: 525\n",
      "Label 12: 950\n",
      "Label 13: 909\n",
      "Label 14: 74\n",
      "Label 15: 450\n",
      "Label 16: 425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16150"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(17):\n",
    "    print(f'Label {i}: {len(X[y == i])}')\n",
    "\n",
    "max([len(X[y == i]) for i in range(17)]) * 17\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = SMOTE(random_state=seed, k_neighbors=2)\n",
    "# X_res, y_res = sm.fit_resample(X.reshape(X.shape[0], -1), y)\n",
    "# print(X_res.shape, y_res.shape)\n",
    "\n",
    "# #fig, axis = plt.subplots(17, 5, figsize=(12, 48))\n",
    "# for label in range(17):\n",
    "#     for i in range(5):\n",
    "#         axis[label, i].imshow(X_res[y_res == label][949-i].reshape(20, 20), vmin=0, vmax=255, cmap='gray')\n",
    "#         axis[label, i].set_xticks([])\n",
    "#         axis[label, i].set_yticks([])\n",
    "#         axis[label, i].set_title(f'Label {label}')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9834, 400) (9834,)\n",
      "(5, 400) (5,)\n"
     ]
    }
   ],
   "source": [
    "def generate_label(X, y, label, n):\n",
    "    \"\"\"generates n augmented images for a given label\"\"\"\n",
    "    X = X[y == label]\n",
    "    y = y[y == label]\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    X_reshaped = X.reshape(X.shape[0], 20, 20, 1)\n",
    "\n",
    "    augmented_data = datagen.flow(X_reshaped, y, batch_size=1, seed=seed)\n",
    "    X_augs, y_augs = [], []\n",
    "    for i in range(n):\n",
    "        X_aug, y_aug = augmented_data.__next__()\n",
    "        X_aug = X_aug.flatten()\n",
    "        X_augs.append(X_aug)\n",
    "        y_augs.append(y_aug)\n",
    "    \n",
    "    X_augs = np.array(X_augs)\n",
    "    y_augs = np.array(y_augs).reshape(-1)\n",
    "\n",
    "    return np.array(X_augs), np.array(y_augs)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "X_aug, y_aug = generate_label(X, y, 1, 5)\n",
    "print(X_aug.shape, y_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16150, 400) (16150,)\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import generate_balanced_data\n",
    "\n",
    "X_aug, y_aug = generate_balanced_data(X, y, 42)\n",
    "\n",
    "print(X_aug.shape, y_aug.shape)\n",
    "# fig, axis = plt.subplots(17, 5, figsize=(12, 48))\n",
    "\n",
    "# for label in range(0,17):\n",
    "#     for i in range(5):\n",
    "#         axis[label, i].imshow(X_aug[y_aug == label][i].reshape(20, 20), vmin=0, vmax=255, cmap='gray')\n",
    "#         axis[label, i].set_xticks([])\n",
    "#         axis[label, i].set_yticks([])\n",
    "#         axis[label, i].set_title(f'Label {label}')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(935, 400)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../data/corrupt_dataset.npz',)\n",
    "CX = dataset['X']\n",
    "print(CX.shape)\n",
    "# fig, axis = plt.subplots(17, 5, figsize=(12, 48))\n",
    "\n",
    "# for label in range(0,17):\n",
    "#     for i in range(5):\n",
    "#         axis[label, i].imshow(X[i+label+5].reshape(20, 20), vmin=0, vmax=255, cmap='gray')\n",
    "#         axis[label, i].set_xticks([])\n",
    "#         axis[label, i].set_yticks([])\n",
    "#         axis[label, i].set_title(f'Label {label}')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, numChannels, classes):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        #scales it down to 18x18 x 20\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=numChannels, \n",
    "            out_channels=20,\n",
    "            kernel_size=(3,3), \n",
    "            )\n",
    "        \n",
    "        #first relu pass\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        #scales it down to 9x9 x 20\n",
    "        self.maxpool1 = nn.MaxPool2d(\n",
    "            kernel_size=(2,2),\n",
    "            stride=(2,2)\n",
    "            )\n",
    "\n",
    "        #scales it down to 7x7 x 50 \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=20, \n",
    "            out_channels=50,\n",
    "            kernel_size=(3,3), \n",
    "            )\n",
    "\n",
    "        #second relu pass\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        #scales it down to 3x3 x 50\n",
    "        self.maxpool2 = nn.MaxPool2d(\n",
    "            kernel_size=(2,2),\n",
    "            stride=(2,2),\n",
    "            )\n",
    "\n",
    "        #takes the 3x3x50 = 450\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=450,\n",
    "            out_features=500,\n",
    "            )\n",
    "        \n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=500,\n",
    "            out_features=classes,\n",
    "        )\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        output = self.logsoftmax(x)\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "TRAIN_SPLIT = 0.70\n",
    "TEST_SPLIT = 1 - TRAIN_SPLIT\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the network now\n",
      "--- Epoch 1/10 - time: 5.23s ---\n",
      "Train loss: 0.0534, Train accuracy: 0.6308\n",
      "Val loss: 0.0209, Val accuracy: 0.7945\n",
      "\n",
      "--- Epoch 2/10 - time: 5.66s ---\n",
      "Train loss: 0.0162, Train accuracy: 0.8367\n",
      "Val loss: 0.0135, Val accuracy: 0.8655\n",
      "\n",
      "--- Epoch 3/10 - time: 7.35s ---\n",
      "Train loss: 0.0109, Train accuracy: 0.8877\n",
      "Val loss: 0.0126, Val accuracy: 0.8820\n",
      "\n",
      "--- Epoch 4/10 - time: 7.54s ---\n",
      "Train loss: 0.0085, Train accuracy: 0.9086\n",
      "Val loss: 0.0107, Val accuracy: 0.8956\n",
      "\n",
      "--- Epoch 5/10 - time: 6.27s ---\n",
      "Train loss: 0.0066, Train accuracy: 0.9278\n",
      "Val loss: 0.0086, Val accuracy: 0.9162\n",
      "\n",
      "--- Epoch 6/10 - time: 6.80s ---\n",
      "Train loss: 0.0055, Train accuracy: 0.9410\n",
      "Val loss: 0.0096, Val accuracy: 0.9125\n",
      "\n",
      "--- Epoch 7/10 - time: 7.54s ---\n",
      "Train loss: 0.0044, Train accuracy: 0.9501\n",
      "Val loss: 0.0108, Val accuracy: 0.9121\n",
      "\n",
      "--- Epoch 8/10 - time: 7.54s ---\n",
      "Train loss: 0.0041, Train accuracy: 0.9565\n",
      "Val loss: 0.0113, Val accuracy: 0.9137\n",
      "\n",
      "--- Epoch 9/10 - time: 5.79s ---\n",
      "Train loss: 0.0037, Train accuracy: 0.9613\n",
      "Val loss: 0.0094, Val accuracy: 0.9187\n",
      "\n",
      "--- Epoch 10/10 - time: 6.92s ---\n",
      "Train loss: 0.0033, Train accuracy: 0.9655\n",
      "Val loss: 0.0107, Val accuracy: 0.9125\n",
      "\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "    X_aug, \n",
    "    y_aug, \n",
    "    test_size=TEST_SPLIT, \n",
    "    random_state=seed\n",
    "    )\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val_test, \n",
    "    y_val_test, \n",
    "    test_size=0.5, \n",
    "    random_state=seed\n",
    "    )\n",
    "\n",
    "from preprocessing import generate_balanced_data\n",
    "\n",
    "X_train, y_train = generate_balanced_data(X_train, y_train, 42)\n",
    "\n",
    "X_train = X_train.reshape(-1, 1, 20, 20)\n",
    "X_val = X_val.reshape(-1, 1, 20, 20)\n",
    "X_test = X_test.reshape(-1, 1, 20, 20)\n",
    "\n",
    "# this is doen so that the values are on the cpu already\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "model = LeNet(numChannels=1, classes=17).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=INIT_LR)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "H = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"training the network now\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    correct_train = 0\n",
    "    correct_val = 0\n",
    "\n",
    "    for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        correct_train += (y_pred.argmax(1) == y_batch).type(torch.float).sum().item()\n",
    "        # print(y_pred)\n",
    "        # print(\"//\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            y_val_pred = model(X_val_batch)\n",
    "            loss = criterion(y_val_pred, y_val_batch)\n",
    "            total_val_loss += loss.item()\n",
    "            correct_val += (y_val_pred.argmax(1) == y_val_batch).type(torch.float).sum().item()\n",
    "         \n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "    train_acc = correct_train / len(train_loader.dataset)\n",
    "    val_acc = correct_val / len(val_loader.dataset)\n",
    "\n",
    "    H['train_loss'].append(avg_train_loss)\n",
    "    H['val_loss'].append(avg_val_loss)\n",
    "    H['train_acc'].append(train_acc)\n",
    "    H['val_acc'].append(val_acc)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"--- Epoch {epoch+1}/{EPOCHS} - time: {end-start:.2f}s ---\")\n",
    "    print(f\"Train loss: {avg_train_loss:.4f}, Train accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val loss: {avg_val_loss:.4f}, Val accuracy: {val_acc:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"training complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.89       140\n",
      "           1       0.94      0.99      0.96       136\n",
      "           2       0.93      0.92      0.92       146\n",
      "           3       0.98      0.95      0.96       164\n",
      "           4       0.98      0.80      0.88       153\n",
      "           5       0.86      0.95      0.90       138\n",
      "           6       0.96      0.96      0.96       145\n",
      "           7       0.94      0.96      0.95       137\n",
      "           8       0.92      0.77      0.84       129\n",
      "           9       0.93      0.86      0.89       147\n",
      "          10       0.90      0.91      0.91       157\n",
      "          11       0.74      0.91      0.82       151\n",
      "          12       0.91      0.93      0.92       137\n",
      "          13       0.94      0.88      0.91       132\n",
      "          14       0.93      0.77      0.84       128\n",
      "          15       0.80      0.90      0.84       139\n",
      "          16       0.99      1.00      1.00       144\n",
      "\n",
      "    accuracy                           0.91      2423\n",
      "   macro avg       0.91      0.90      0.91      2423\n",
      "weighted avg       0.91      0.91      0.91      2423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        preds.append(y_pred.argmax(1))\n",
    "\n",
    "    preds = torch.cat(preds).cpu().numpy()\n",
    "\n",
    "print(classification_report(y_test.cpu().numpy(), preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
