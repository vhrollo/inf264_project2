{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 11:05:50.057141: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-06 11:05:52.659007: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff7ec51ad70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9834, 400) (9834,)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../data/dataset.npz',)\n",
    "X, y = dataset['X'], dataset['y']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axis = plt.subplots(17, 5, figsize=(12, 48))\n",
    "\n",
    "# for label in range(17):\n",
    "#     for i in range(5):\n",
    "#         axis[label, i].imshow(X[y == label][i+5].reshape(20, 20), vmin=0, vmax=255, cmap='gray')\n",
    "#         axis[label, i].set_xticks([])\n",
    "#         axis[label, i].set_yticks([])\n",
    "#         axis[label, i].set_title(f'Label {label}')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 627\n",
      "Label 1: 230\n",
      "Label 2: 525\n",
      "Label 3: 950\n",
      "Label 4: 500\n",
      "Label 5: 695\n",
      "Label 6: 912\n",
      "Label 7: 605\n",
      "Label 8: 427\n",
      "Label 9: 205\n",
      "Label 10: 825\n",
      "Label 11: 525\n",
      "Label 12: 950\n",
      "Label 13: 909\n",
      "Label 14: 74\n",
      "Label 15: 450\n",
      "Label 16: 425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16150"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(17):\n",
    "    print(f'Label {i}: {len(X[y == i])}')\n",
    "\n",
    "max([len(X[y == i]) for i in range(17)]) * 17\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = SMOTE(random_state=seed, k_neighbors=2)\n",
    "# X_res, y_res = sm.fit_resample(X.reshape(X.shape[0], -1), y)\n",
    "# print(X_res.shape, y_res.shape)\n",
    "\n",
    "# #fig, axis = plt.subplots(17, 5, figsize=(12, 48))\n",
    "# for label in range(17):\n",
    "#     for i in range(5):\n",
    "#         axis[label, i].imshow(X_res[y_res == label][949-i].reshape(20, 20), vmin=0, vmax=255, cmap='gray')\n",
    "#         axis[label, i].set_xticks([])\n",
    "#         axis[label, i].set_yticks([])\n",
    "#         axis[label, i].set_title(f'Label {label}')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9834, 400) (9834,)\n",
      "(5, 400) (5,)\n"
     ]
    }
   ],
   "source": [
    "def generate_label(X, y, label, n):\n",
    "    \"\"\"generates n augmented images for a given label\"\"\"\n",
    "    X = X[y == label]\n",
    "    y = y[y == label]\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    X_reshaped = X.reshape(X.shape[0], 20, 20, 1)\n",
    "\n",
    "    augmented_data = datagen.flow(X_reshaped, y, batch_size=1, seed=seed)\n",
    "    X_augs, y_augs = [], []\n",
    "    for i in range(n):\n",
    "        X_aug, y_aug = augmented_data.__next__()\n",
    "        X_aug = X_aug.flatten()\n",
    "        X_augs.append(X_aug)\n",
    "        y_augs.append(y_aug)\n",
    "    \n",
    "    X_augs = np.array(X_augs)\n",
    "    y_augs = np.array(y_augs).reshape(-1)\n",
    "\n",
    "    return np.array(X_augs), np.array(y_augs)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "X_aug, y_aug = generate_label(X, y, 1, 5)\n",
    "print(X_aug.shape, y_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16150, 400) (16150,)\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import generate_balanced_data\n",
    "\n",
    "X_aug, y_aug = generate_balanced_data(X, y, 42)\n",
    "\n",
    "print(X_aug.shape, y_aug.shape)\n",
    "# fig, axis = plt.subplots(17, 5, figsize=(12, 48))\n",
    "\n",
    "# for label in range(0,17):\n",
    "#     for i in range(5):\n",
    "#         axis[label, i].imshow(X_aug[y_aug == label][i].reshape(20, 20), vmin=0, vmax=255, cmap='gray')\n",
    "#         axis[label, i].set_xticks([])\n",
    "#         axis[label, i].set_yticks([])\n",
    "#         axis[label, i].set_title(f'Label {label}')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(935, 400)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../data/corrupt_dataset.npz',)\n",
    "CX = dataset['X']\n",
    "print(CX.shape)\n",
    "# fig, axis = plt.subplots(17, 5, figsize=(12, 48))\n",
    "\n",
    "# for label in range(0,17):\n",
    "#     for i in range(5):\n",
    "#         axis[label, i].imshow(X[i+label+5].reshape(20, 20), vmin=0, vmax=255, cmap='gray')\n",
    "#         axis[label, i].set_xticks([])\n",
    "#         axis[label, i].set_yticks([])\n",
    "#         axis[label, i].set_title(f'Label {label}')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, numChannels, classes):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        #scales it down to 18x18 x 20\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=numChannels, \n",
    "            out_channels=20,\n",
    "            kernel_size=(3,3), \n",
    "            )\n",
    "        \n",
    "        #first relu pass\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        #scales it down to 9x9 x 20\n",
    "        self.maxpool1 = nn.MaxPool2d(\n",
    "            kernel_size=(2,2),\n",
    "            stride=(2,2)\n",
    "            )\n",
    "\n",
    "        #scales it down to 7x7 x 50 \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=20, \n",
    "            out_channels=50,\n",
    "            kernel_size=(3,3), \n",
    "            )\n",
    "\n",
    "        #second relu pass\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        #scales it down to 3x3 x 50\n",
    "        self.maxpool2 = nn.MaxPool2d(\n",
    "            kernel_size=(2,2),\n",
    "            stride=(2,2),\n",
    "            )\n",
    "\n",
    "        #takes the 3x3x50 = 450\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=450,\n",
    "            out_features=500,\n",
    "            )\n",
    "        \n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=500,\n",
    "            out_features=classes,\n",
    "        )\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        output = self.logsoftmax(x)\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "class AlexNetEsque(nn.Module):\n",
    "    def __init__(self, numChannels, classes):\n",
    "        super(AlexNetEsque, self).__init__()\n",
    "\n",
    "        # 20 x 20 x 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=numChannels, \n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # 10 x 10 x 64\n",
    "        self.maxpool1 = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        # 10 x 10 x 128\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=64, \n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # 5 x 5 x 128\n",
    "        self.maxpool2 = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        # 5 x 5 x 264\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=128, \n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # 5 x 5 x 256\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=256, \n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        # 5 x 5 x 128\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=256, \n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        # 2x2 x 128\n",
    "        self.maxpool3 = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        # 2x2 x 128 = 512 \n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=512,\n",
    "            out_features=512\n",
    "        )\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        # helps with generalization\n",
    "        # basically means if you run longer,\n",
    "        # it gets better\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=512,\n",
    "            out_features=256\n",
    "        )\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc3 = nn.Linear(\n",
    "            in_features=256,\n",
    "            out_features=classes\n",
    "        )\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu7(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        output = self.logsoftmax(x)\n",
    "        return output\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "TRAIN_SPLIT = 0.70\n",
    "TEST_SPLIT = 1 - TRAIN_SPLIT\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the network now\n",
      "--- Epoch 1/10 - time: 13.20s ---\n",
      "Train loss: 0.0519, Train accuracy: 0.4860\n",
      "Val loss: 0.0177, Val accuracy: 0.8264\n",
      "\n",
      "--- Epoch 2/10 - time: 6.42s ---\n",
      "Train loss: 0.0168, Train accuracy: 0.8320\n",
      "Val loss: 0.0086, Val accuracy: 0.9173\n",
      "\n",
      "--- Epoch 3/10 - time: 6.94s ---\n",
      "Train loss: 0.0107, Train accuracy: 0.8921\n",
      "Val loss: 0.0072, Val accuracy: 0.9268\n",
      "\n",
      "--- Epoch 4/10 - time: 8.08s ---\n",
      "Train loss: 0.0079, Train accuracy: 0.9182\n",
      "Val loss: 0.0100, Val accuracy: 0.8936\n",
      "\n",
      "--- Epoch 5/10 - time: 12.25s ---\n",
      "Train loss: 0.0063, Train accuracy: 0.9371\n",
      "Val loss: 0.0056, Val accuracy: 0.9390\n",
      "\n",
      "--- Epoch 6/10 - time: 8.49s ---\n",
      "Train loss: 0.0051, Train accuracy: 0.9460\n",
      "Val loss: 0.0049, Val accuracy: 0.9451\n",
      "\n",
      "--- Epoch 7/10 - time: 7.95s ---\n",
      "Train loss: 0.0043, Train accuracy: 0.9551\n",
      "Val loss: 0.0049, Val accuracy: 0.9519\n",
      "\n",
      "--- Epoch 8/10 - time: 8.54s ---\n",
      "Train loss: 0.0031, Train accuracy: 0.9669\n",
      "Val loss: 0.0048, Val accuracy: 0.9525\n",
      "\n",
      "--- Epoch 9/10 - time: 5.64s ---\n",
      "Train loss: 0.0028, Train accuracy: 0.9696\n",
      "Val loss: 0.0044, Val accuracy: 0.9593\n",
      "\n",
      "--- Epoch 10/10 - time: 5.35s ---\n",
      "Train loss: 0.0020, Train accuracy: 0.9803\n",
      "Val loss: 0.0047, Val accuracy: 0.9512\n",
      "\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import generate_balanced_data, scale_data, scale_data10\n",
    "\n",
    "X = scale_data10(X)\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=TEST_SPLIT, \n",
    "    random_state=seed\n",
    "    )\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val_test, \n",
    "    y_val_test, \n",
    "    test_size=0.5, \n",
    "    random_state=seed\n",
    "    )\n",
    "\n",
    "X_train, y_train = generate_balanced_data(X_train, y_train, 42)\n",
    "\n",
    "X_train = X_train.reshape(-1, 1, 20, 20)\n",
    "X_val = X_val.reshape(-1, 1, 20, 20)\n",
    "X_test = X_test.reshape(-1, 1, 20, 20)\n",
    "\n",
    "# this is doen so that the values are on the cpu already\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    worker_init_fn = lambda _: np.random.seed(seed)\n",
    "    )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    worker_init_fn = lambda _: np.random.seed(seed)\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    worker_init_fn = lambda _: np.random.seed(seed)\n",
    "    )\n",
    "\n",
    "model = LeNet(numChannels=1, classes=17).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=INIT_LR, momentum=MOMENTUM)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "H = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"training the network now\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    correct_train = 0\n",
    "    correct_val = 0\n",
    "\n",
    "    for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        correct_train += (y_pred.argmax(1) == y_batch).type(torch.float).sum().item()\n",
    "        # print(y_pred)\n",
    "        # print(\"//\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            y_val_pred = model(X_val_batch)\n",
    "            loss = criterion(y_val_pred, y_val_batch)\n",
    "            total_val_loss += loss.item()\n",
    "            correct_val += (y_val_pred.argmax(1) == y_val_batch).type(torch.float).sum().item()\n",
    "         \n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "    train_acc = correct_train / len(train_loader.dataset)\n",
    "    val_acc = correct_val / len(val_loader.dataset)\n",
    "\n",
    "    H['train_loss'].append(avg_train_loss)\n",
    "    H['val_loss'].append(avg_val_loss)\n",
    "    H['train_acc'].append(train_acc)\n",
    "    H['val_acc'].append(val_acc)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"--- Epoch {epoch+1}/{EPOCHS} - time: {end-start:.2f}s ---\")\n",
    "    print(f\"Train loss: {avg_train_loss:.4f}, Train accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val loss: {avg_val_loss:.4f}, Val accuracy: {val_acc:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"training complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        82\n",
      "           1       0.97      0.97      0.97        38\n",
      "           2       0.99      1.00      0.99        88\n",
      "           3       0.98      0.97      0.98       123\n",
      "           4       0.97      0.97      0.97        80\n",
      "           5       0.98      0.99      0.99       112\n",
      "           6       1.00      0.96      0.98       141\n",
      "           7       0.99      1.00      0.99        95\n",
      "           8       0.97      0.95      0.96        62\n",
      "           9       1.00      0.94      0.97        33\n",
      "          10       0.98      0.96      0.97       137\n",
      "          11       0.83      0.93      0.88        72\n",
      "          12       0.99      0.98      0.99       133\n",
      "          13       0.96      0.94      0.95       129\n",
      "          14       1.00      0.78      0.88         9\n",
      "          15       0.87      0.97      0.92        69\n",
      "          16       1.00      1.00      1.00        73\n",
      "\n",
      "    accuracy                           0.97      1476\n",
      "   macro avg       0.97      0.96      0.96      1476\n",
      "weighted avg       0.97      0.97      0.97      1476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        preds.append(y_pred.argmax(1))\n",
    "\n",
    "    preds = torch.cat(preds).cpu().numpy()\n",
    "\n",
    "print(classification_report(y_test.cpu().numpy(), preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
