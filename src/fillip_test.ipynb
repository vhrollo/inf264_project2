{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9834, 400) (9834,)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../data/dataset.npz',)\n",
    "X, y = dataset['X'], dataset['y']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axis = plt.subplots(17, 5, figsize=(12, 48))\n",
    "\n",
    "# for label in range(17):\n",
    "#     for i in range(5):\n",
    "#         axis[label, i].imshow(X[y == label][i+5].reshape(20, 20), vmin=0, vmax=255, cmap='gray')\n",
    "#         axis[label, i].set_xticks([])\n",
    "#         axis[label, i].set_yticks([])\n",
    "#         axis[label, i].set_title(f'Label {label}')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 627\n",
      "Label 1: 230\n",
      "Label 2: 525\n",
      "Label 3: 950\n",
      "Label 4: 500\n",
      "Label 5: 695\n",
      "Label 6: 912\n",
      "Label 7: 605\n",
      "Label 8: 427\n",
      "Label 9: 205\n",
      "Label 10: 825\n",
      "Label 11: 525\n",
      "Label 12: 950\n",
      "Label 13: 909\n",
      "Label 14: 74\n",
      "Label 15: 450\n",
      "Label 16: 425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16150"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(17):\n",
    "    print(f'Label {i}: {len(X[y == i])}')\n",
    "\n",
    "max([len(X[y == i]) for i in range(17)]) * 17\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = SMOTE(random_state=seed, k_neighbors=2)\n",
    "# X_res, y_res = sm.fit_resample(X.reshape(X.shape[0], -1), y)\n",
    "# print(X_res.shape, y_res.shape)\n",
    "\n",
    "# #fig, axis = plt.subplots(17, 5, figsize=(12, 48))\n",
    "# for label in range(17):\n",
    "#     for i in range(5):\n",
    "#         axis[label, i].imshow(X_res[y_res == label][949-i].reshape(20, 20), vmin=0, vmax=255, cmap='gray')\n",
    "#         axis[label, i].set_xticks([])\n",
    "#         axis[label, i].set_yticks([])\n",
    "#         axis[label, i].set_title(f'Label {label}')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9834, 400) (9834,)\n",
      "(5, 400) (5,)\n"
     ]
    }
   ],
   "source": [
    "def generate_label(X, y, label, n):\n",
    "    \"\"\"generates n augmented images for a given label\"\"\"\n",
    "    X = X[y == label]\n",
    "    y = y[y == label]\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    X_reshaped = X.reshape(X.shape[0], 20, 20, 1)\n",
    "\n",
    "    augmented_data = datagen.flow(X_reshaped, y, batch_size=1, seed=seed)\n",
    "    X_augs, y_augs = [], []\n",
    "    for i in range(n):\n",
    "        X_aug, y_aug = augmented_data.__next__()\n",
    "        X_aug = X_aug.flatten()\n",
    "        X_augs.append(X_aug)\n",
    "        y_augs.append(y_aug)\n",
    "    \n",
    "    X_augs = np.array(X_augs)\n",
    "    y_augs = np.array(y_augs).reshape(-1)\n",
    "\n",
    "    return np.array(X_augs), np.array(y_augs)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "X_aug, y_aug = generate_label(X, y, 1, 5)\n",
    "print(X_aug.shape, y_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16150, 400) (16150,)\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import generate_balanced_data\n",
    "\n",
    "X_aug, y_aug = generate_balanced_data(X, y, 42)\n",
    "\n",
    "print(X_aug.shape, y_aug.shape)\n",
    "# fig, axis = plt.subplots(17, 5, figsize=(12, 48))\n",
    "\n",
    "# for label in range(0,17):\n",
    "#     for i in range(5):\n",
    "#         axis[label, i].imshow(X_aug[y_aug == label][i].reshape(20, 20), vmin=0, vmax=255, cmap='gray')\n",
    "#         axis[label, i].set_xticks([])\n",
    "#         axis[label, i].set_yticks([])\n",
    "#         axis[label, i].set_title(f'Label {label}')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(935, 400)\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../data/corrupt_dataset.npz',)\n",
    "CX = dataset['X']\n",
    "print(CX.shape)\n",
    "# fig, axis = plt.subplots(17, 5, figsize=(12, 48))\n",
    "\n",
    "# for label in range(0,17):\n",
    "#     for i in range(5):\n",
    "#         axis[label, i].imshow(X[i+label+5].reshape(20, 20), vmin=0, vmax=255, cmap='gray')\n",
    "#         axis[label, i].set_xticks([])\n",
    "#         axis[label, i].set_yticks([])\n",
    "#         axis[label, i].set_title(f'Label {label}')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, numChannels, classes):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        #scales it down to 18x18 x 20\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=numChannels, \n",
    "            out_channels=20,\n",
    "            kernel_size=(3,3), \n",
    "            )\n",
    "        \n",
    "        #first relu pass\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        #scales it down to 9x9 x 20\n",
    "        self.maxpool1 = nn.MaxPool2d(\n",
    "            kernel_size=(2,2),\n",
    "            stride=(2,2)\n",
    "            )\n",
    "\n",
    "        #scales it down to 7x7 x 50 \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=20, \n",
    "            out_channels=50,\n",
    "            kernel_size=(3,3), \n",
    "            )\n",
    "\n",
    "        #second relu pass\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        #scales it down to 3x3 x 50\n",
    "        self.maxpool2 = nn.MaxPool2d(\n",
    "            kernel_size=(2,2),\n",
    "            stride=(2,2),\n",
    "            )\n",
    "\n",
    "        #takes the 3x3x50 = 450\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=450,\n",
    "            out_features=500,\n",
    "            )\n",
    "        \n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=500,\n",
    "            out_features=classes,\n",
    "        )\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        output = self.logsoftmax(x)\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "class AlexNetEsque(nn.Module):\n",
    "    def __init__(self, numChannels, classes):\n",
    "        super(AlexNetEsque, self).__init__()\n",
    "\n",
    "        # 20 x 20 x 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=numChannels, \n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # 10 x 10 x 64\n",
    "        self.maxpool1 = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        # 10 x 10 x 128\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=64, \n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # 5 x 5 x 128\n",
    "        self.maxpool2 = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        # 5 x 5 x 264\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=128, \n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # 5 x 5 x 256\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=256, \n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        # 5 x 5 x 128\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=256, \n",
    "            out_channels=128,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        # 2x2 x 128\n",
    "        self.maxpool3 = nn.MaxPool2d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "\n",
    "        # 2x2 x 128 = 512 \n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=512,\n",
    "            out_features=512\n",
    "        )\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        # helps with generalization\n",
    "        # basically means if you run longer,\n",
    "        # it gets better\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=512,\n",
    "            out_features=256\n",
    "        )\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc3 = nn.Linear(\n",
    "            in_features=256,\n",
    "            out_features=classes\n",
    "        )\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu7(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        output = self.logsoftmax(x)\n",
    "        return output\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "\n",
    "TRAIN_SPLIT = 0.70\n",
    "TEST_SPLIT = 1 - TRAIN_SPLIT\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the network now\n",
      "--- Epoch 1/20 - time: 9.51s ---\n",
      "Train loss: 0.0888, Train accuracy: 0.0596\n",
      "Val loss: 0.0903, Val accuracy: 0.0671\n",
      "\n",
      "--- Epoch 2/20 - time: 10.56s ---\n",
      "Train loss: 0.0858, Train accuracy: 0.1013\n",
      "Val loss: 0.0835, Val accuracy: 0.1105\n",
      "\n",
      "--- Epoch 3/20 - time: 11.20s ---\n",
      "Train loss: 0.0680, Train accuracy: 0.2754\n",
      "Val loss: 0.0427, Val accuracy: 0.5349\n",
      "\n",
      "--- Epoch 4/20 - time: 11.06s ---\n",
      "Train loss: 0.0372, Train accuracy: 0.5993\n",
      "Val loss: 0.0227, Val accuracy: 0.7688\n",
      "\n",
      "--- Epoch 5/20 - time: 11.21s ---\n",
      "Train loss: 0.0188, Train accuracy: 0.8153\n",
      "Val loss: 0.0091, Val accuracy: 0.9092\n",
      "\n",
      "--- Epoch 6/20 - time: 11.06s ---\n",
      "Train loss: 0.0116, Train accuracy: 0.8874\n",
      "Val loss: 0.0059, Val accuracy: 0.9464\n",
      "\n",
      "--- Epoch 7/20 - time: 10.91s ---\n",
      "Train loss: 0.0081, Train accuracy: 0.9196\n",
      "Val loss: 0.0051, Val accuracy: 0.9464\n",
      "\n",
      "--- Epoch 8/20 - time: 11.05s ---\n",
      "Train loss: 0.0067, Train accuracy: 0.9369\n",
      "Val loss: 0.0075, Val accuracy: 0.9390\n",
      "\n",
      "--- Epoch 9/20 - time: 11.21s ---\n",
      "Train loss: 0.0066, Train accuracy: 0.9371\n",
      "Val loss: 0.0041, Val accuracy: 0.9607\n",
      "\n",
      "--- Epoch 10/20 - time: 84.42s ---\n",
      "Train loss: 0.0043, Train accuracy: 0.9579\n",
      "Val loss: 0.0037, Val accuracy: 0.9668\n",
      "\n",
      "--- Epoch 11/20 - time: 9.91s ---\n",
      "Train loss: 0.0036, Train accuracy: 0.9652\n",
      "Val loss: 0.0048, Val accuracy: 0.9607\n",
      "\n",
      "--- Epoch 12/20 - time: 12.59s ---\n",
      "Train loss: 0.0031, Train accuracy: 0.9692\n",
      "Val loss: 0.0035, Val accuracy: 0.9681\n",
      "\n",
      "--- Epoch 13/20 - time: 13.60s ---\n",
      "Train loss: 0.0024, Train accuracy: 0.9768\n",
      "Val loss: 0.0032, Val accuracy: 0.9736\n",
      "\n",
      "--- Epoch 14/20 - time: 12.63s ---\n",
      "Train loss: 0.0020, Train accuracy: 0.9803\n",
      "Val loss: 0.0048, Val accuracy: 0.9661\n",
      "\n",
      "--- Epoch 15/20 - time: 12.75s ---\n",
      "Train loss: 0.0015, Train accuracy: 0.9844\n",
      "Val loss: 0.0035, Val accuracy: 0.9736\n",
      "\n",
      "--- Epoch 16/20 - time: 12.86s ---\n",
      "Train loss: 0.0016, Train accuracy: 0.9836\n",
      "Val loss: 0.0031, Val accuracy: 0.9736\n",
      "\n",
      "--- Epoch 17/20 - time: 12.91s ---\n",
      "Train loss: 0.0013, Train accuracy: 0.9882\n",
      "Val loss: 0.0035, Val accuracy: 0.9763\n",
      "\n",
      "--- Epoch 18/20 - time: 12.80s ---\n",
      "Train loss: 0.0011, Train accuracy: 0.9905\n",
      "Val loss: 0.0033, Val accuracy: 0.9749\n",
      "\n",
      "--- Epoch 19/20 - time: 12.72s ---\n",
      "Train loss: 0.0011, Train accuracy: 0.9898\n",
      "Val loss: 0.0044, Val accuracy: 0.9736\n",
      "\n",
      "--- Epoch 20/20 - time: 12.99s ---\n",
      "Train loss: 0.0010, Train accuracy: 0.9900\n",
      "Val loss: 0.0035, Val accuracy: 0.9722\n",
      "\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import generate_balanced_data, scale_data, scale_data10, augment_brightness_contrast_tensor\n",
    "\n",
    "X = scale_data10(X)\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=TEST_SPLIT, \n",
    "    random_state=seed\n",
    "    )\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val_test, \n",
    "    y_val_test, \n",
    "    test_size=0.5, \n",
    "    random_state=seed\n",
    "    )\n",
    "\n",
    "X_train, y_train = generate_balanced_data(X_train, y_train, 42)\n",
    "\n",
    "X_train = X_train.reshape(-1, 1, 20, 20)\n",
    "X_val = X_val.reshape(-1, 1, 20, 20)\n",
    "X_test = X_test.reshape(-1, 1, 20, 20)\n",
    "\n",
    "# this is doen so that the values are on the cpu already\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "model = AlexNetEsque(numChannels=1, classes=17).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "H = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"training the network now\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    correct_train = 0\n",
    "    correct_val = 0\n",
    "\n",
    "    for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        correct_train += (y_pred.argmax(1) == y_batch).type(torch.float).sum().item()\n",
    "        # print(y_pred)\n",
    "        # print(\"//\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            y_val_pred = model(X_val_batch)\n",
    "            loss = criterion(y_val_pred, y_val_batch)\n",
    "            total_val_loss += loss.item()\n",
    "            correct_val += (y_val_pred.argmax(1) == y_val_batch).type(torch.float).sum().item()\n",
    "         \n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "    train_acc = correct_train / len(train_loader.dataset)\n",
    "    val_acc = correct_val / len(val_loader.dataset)\n",
    "\n",
    "    H['train_loss'].append(avg_train_loss)\n",
    "    H['val_loss'].append(avg_val_loss)\n",
    "    H['train_acc'].append(train_acc)\n",
    "    H['val_acc'].append(val_acc)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"--- Epoch {epoch+1}/{EPOCHS} - time: {end-start:.2f}s ---\")\n",
    "    print(f\"Train loss: {avg_train_loss:.4f}, Train accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val loss: {avg_val_loss:.4f}, Val accuracy: {val_acc:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"training complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94        82\n",
      "           1       1.00      0.97      0.99        38\n",
      "           2       0.97      0.98      0.97        88\n",
      "           3       1.00      0.96      0.98       123\n",
      "           4       1.00      0.97      0.99        80\n",
      "           5       0.97      1.00      0.98       112\n",
      "           6       0.99      1.00      1.00       141\n",
      "           7       1.00      0.99      0.99        95\n",
      "           8       1.00      0.95      0.98        62\n",
      "           9       0.97      1.00      0.99        33\n",
      "          10       0.97      1.00      0.99       137\n",
      "          11       0.96      0.97      0.97        72\n",
      "          12       0.99      0.99      0.99       133\n",
      "          13       0.95      0.96      0.96       129\n",
      "          14       1.00      0.78      0.88         9\n",
      "          15       0.97      1.00      0.99        69\n",
      "          16       1.00      1.00      1.00        73\n",
      "\n",
      "    accuracy                           0.98      1476\n",
      "   macro avg       0.98      0.97      0.97      1476\n",
      "weighted avg       0.98      0.98      0.98      1476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        preds.append(y_pred.argmax(1))\n",
    "\n",
    "    preds = torch.cat(preds).cpu().numpy()\n",
    "\n",
    "print(classification_report(y_test.cpu().numpy(), preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
